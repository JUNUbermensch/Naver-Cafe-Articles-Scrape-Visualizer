{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "526128d3-2e34-483a-9f67-f49ef8866db6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_agent = ''\n",
    "# Type your user agent in \"\"\n",
    "# your user agent can be found here: https://www.whatismybrowser.com/detect/what-is-my-user-agent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd586d6-2d12-495e-a17b-e66b9bf2fc39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import Counter\n",
    "from konlpy.tag import Hannanum\n",
    "from tqdm import tqdm\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41fab1d1-f59d-4306-9fff-b85afc4b6484",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 114/114 [00:00<00:00, 56720.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1049 scraped successfully with 8 articles.\n",
      "Page 414 scraped successfully with 1 articles.\n",
      "Page 836 scraped successfully with 0 articles.\n",
      "Page 839 scraped successfully with 0 articles.\n",
      "Page 805 scraped successfully with 0 articles.\n",
      "Page 828 scraped successfully with 0 articles.\n",
      "Page 802 scraped successfully with 1 articles.\n",
      "Page 803 scraped successfully with 0 articles.\n",
      "Page 799 scraped successfully with 1 articles.\n",
      "Page 831 scraped successfully with 0 articles.\n",
      "Page 804 scraped successfully with 0 articles.\n",
      "Page 815 scraped successfully with 0 articles.\n",
      "Page 807 scraped successfully with 0 articles.\n",
      "Page 827 scraped successfully with 0 articles.\n",
      "Page 814 scraped successfully with 0 articles.\n",
      "Page 801 scraped successfully with 1 articles.\n",
      "Page 816 scraped successfully with 0 articles.\n",
      "Page 806 scraped successfully with 0 articles.\n",
      "Page 819 scraped successfully with 0 articles.\n",
      "Page 1121 scraped successfully with 8 articles.\n",
      "Page 821 scraped successfully with 0 articles.\n",
      "Page 568 scraped successfully with 0 articles.\n",
      "Page 854 scraped successfully with 0 articles.\n",
      "Page 1080 scraped successfully with 8 articles.\n",
      "Page 823 scraped successfully with 0 articles.\n",
      "Page 1081 scraped successfully with 8 articles.\n",
      "Page 811 scraped successfully with 0 articles.\n",
      "Page 824 scraped successfully with 0 articles.\n",
      "Page 1087 scraped successfully with 8 articles.\n",
      "Page 809 scraped successfully with 1 articles.\n",
      "Page 817 scraped successfully with 1 articles.\n",
      "Page 1091 scraped successfully with 8 articles.\n",
      "Page 1078 scraped successfully with 8 articles.\n",
      "Page 808 scraped successfully with 0 articles.\n",
      "Page 1074 scraped successfully with 8 articles.\n",
      "Page 1082 scraped successfully with 8 articles.\n",
      "Page 1083 scraped successfully with 8 articles.\n",
      "Page 798 scraped successfully with 0 articles.\n",
      "Page 712 scraped successfully with 0 articles.\n",
      "Page 1086 scraped successfully with 8 articles.\n",
      "Page 1089 scraped successfully with 8 articles.\n",
      "Page 832 scraped successfully with 1 articles.\n",
      "Page 810 scraped successfully with 0 articles.\n",
      "Page 1113 scraped successfully with 8 articles.\n",
      "Page 1092 scraped successfully with 8 articles.\n",
      "Page 1124 scraped successfully with 8 articles.\n",
      "Page 800 scraped successfully with 0 articles.\n",
      "Page 1135 scraped successfully with 8 articles.\n",
      "Page 825 scraped successfully with 0 articles.\n",
      "Page 1108 scraped successfully with 8 articles.\n",
      "Page 1077 scraped successfully with 8 articles.\n",
      "Page 835 scraped successfully with 0 articles.\n",
      "Page 1126 scraped successfully with 8 articles.\n",
      "Page 1076 scraped successfully with 8 articles.\n",
      "Page 1088 scraped successfully with 8 articles.\n",
      "Page 1125 scraped successfully with 8 articles.\n",
      "Page 858 scraped successfully with 0 articles.\n",
      "Page 1111 scraped successfully with 8 articles.\n",
      "Page 1100 scraped successfully with 8 articles.\n",
      "Page 1075 scraped successfully with 8 articles.\n",
      "Page 1114 scraped successfully with 8 articles.\n",
      "Page 1102 scraped successfully with 8 articles.\n",
      "Page 1107 scraped successfully with 8 articles.\n",
      "Page 1084 scraped successfully with 8 articles.\n",
      "Page 1137 scraped successfully with 8 articles.\n",
      "Page 1079 scraped successfully with 8 articles.\n",
      "Page 1115 scraped successfully with 8 articles.\n",
      "Page 1105 scraped successfully with 8 articles.\n",
      "Page 1085 scraped successfully with 8 articles.\n",
      "Page 840 scraped successfully with 0 articles.\n",
      "Page 1112 scraped successfully with 8 articles.\n",
      "Page 1121 scraped successfully with 8 articles.\n",
      "Page 338 scraped successfully with 0 articles.\n",
      "Page 20 scraped successfully with 0 articles.\n",
      "Page 340 scraped successfully with 0 articles.\n",
      "Page 616 scraped successfully with 0 articles.\n",
      "Page 848 scraped successfully with 0 articles.\n",
      "Page 1123 scraped successfully with 8 articles.\n",
      "Page 22 scraped successfully with 0 articles.\n",
      "Page 26 scraped successfully with 0 articles.\n",
      "Page 578 scraped successfully with 0 articles.\n",
      "Page 574 scraped successfully with 0 articles.\n",
      "Page 25 scraped successfully with 0 articles.\n",
      "Page 576 scraped successfully with 1 articles.\n",
      "Page 573 scraped successfully with 0 articles.\n",
      "Page 575 scraped successfully with 1 articles.\n",
      "Page 615 scraped successfully with 0 articles.\n",
      "Page 854 scraped successfully with 0 articles.\n",
      "Page 850 scraped successfully with 1 articles.\n",
      "Page 585 scraped successfully with 1 articles.\n",
      "Page 1128 scraped successfully with 8 articles.\n",
      "Page 1124 scraped successfully with 8 articles.\n",
      "Page 23 scraped successfully with 0 articles.\n",
      "Page 859 scraped successfully with 0 articles.\n",
      "Page 105 scraped successfully with 1 articles.\n",
      "Page 58 scraped successfully with 0 articles.\n",
      "Page 53 scraped successfully with 0 articles.\n",
      "Page 858 scraped successfully with 0 articles.\n",
      "Page 32 scraped successfully with 2 articles.\n",
      "Page 572 scraped successfully with 0 articles.\n",
      "Page 339 scraped successfully with 0 articles.\n",
      "Page 11 scraped successfully with 0 articles.\n",
      "Page 7 scraped successfully with 0 articles.\n",
      "Page 861 scraped successfully with 1 articles.\n",
      "Page 1126 scraped successfully with 8 articles.\n",
      "Page 28 scraped successfully with 0 articles.\n",
      "Page 103 scraped successfully with 0 articles.\n",
      "Page 1125 scraped successfully with 8 articles.\n",
      "Page 583 scraped successfully with 0 articles.\n",
      "Page 852 scraped successfully with 0 articles.\n",
      "Page 856 scraped successfully with 0 articles.\n",
      "Page 1058 scraped successfully with 8 articles.\n",
      "Page 691 scraped successfully with 0 articles.\n",
      "Page 1132 scraped successfully with 8 articles.\n",
      "Page 580 scraped successfully with 0 articles.\n",
      "Page 41 scraped successfully with 0 articles.\n",
      "Page 1130 scraped successfully with 8 articles.\n",
      "Page 890 scraped successfully with 0 articles.\n",
      "Page 855 scraped successfully with 0 articles.\n",
      "Page 81 scraped successfully with 1 articles.\n",
      "Page 33 scraped successfully with 1 articles.\n",
      "Page 617 scraped successfully with 2 articles.\n",
      "Page 49 scraped successfully with 0 articles.\n",
      "Page 857 scraped successfully with 0 articles.\n",
      "Page 891 scraped successfully with 0 articles.\n",
      "Page 93 scraped successfully with 0 articles.\n",
      "Page 1134 scraped successfully with 8 articles.\n",
      "Page 577 scraped successfully with 1 articles.\n",
      "Page 853 scraped successfully with 0 articles.\n",
      "Page 85 scraped successfully with 0 articles.\n",
      "Page 614 scraped successfully with 0 articles.\n",
      "Page 21 scraped successfully with 1 articles.\n",
      "Page 420 scraped successfully with 0 articles.\n",
      "Page 849 scraped successfully with 2 articles.\n",
      "Page 67 scraped successfully with 0 articles.\n",
      "Page 108 scraped successfully with 0 articles.\n",
      "Page 851 scraped successfully with 0 articles.\n",
      "Page 1076 scraped successfully with 8 articles.\n",
      "Page 1133 scraped successfully with 8 articles.\n",
      "Page 421 scraped successfully with 0 articles.\n",
      "Page 96 scraped successfully with 0 articles.\n",
      "Page 24 scraped successfully with 0 articles.\n",
      "Page 61 scraped successfully with 0 articles.\n",
      "Page 40 scraped successfully with 0 articles.\n",
      "Page 82 scraped successfully with 1 articles.\n",
      "Page 104 scraped successfully with 2 articles.\n",
      "Page 30 scraped successfully with 0 articles.\n",
      "Page 9 scraped successfully with 0 articles.\n",
      "Page 79 scraped successfully with 0 articles.\n",
      "Page 31 scraped successfully with 0 articles.\n",
      "Page 74 scraped successfully with 0 articles.\n",
      "Page 690 scraped successfully with 0 articles.\n",
      "Page 584 scraped successfully with 2 articles.\n",
      "Page 784 scraped successfully with 0 articles.\n",
      "Page 1127 scraped successfully with 8 articles.\n",
      "Page 695 scraped successfully with 0 articles.\n",
      "Page 18 scraped successfully with 0 articles.\n",
      "Page 50 scraped successfully with 0 articles.\n",
      "Page 87 scraped successfully with 0 articles.\n",
      "Page 1129 scraped successfully with 8 articles.\n",
      "Page 1135 scraped successfully with 8 articles.\n",
      "Page 48 scraped successfully with 0 articles.\n",
      "Page 31 scraped successfully with 0 articles.\n",
      "Page 705 scraped successfully with 0 articles.\n",
      "Page 84 scraped successfully with 0 articles.\n",
      "Page 792 scraped successfully with 0 articles.\n",
      "Page 582 scraped successfully with 0 articles.\n",
      "Page 416 scraped successfully with 0 articles.\n",
      "Page 860 scraped successfully with 0 articles.\n",
      "Page 581 scraped successfully with 0 articles.\n",
      "Page 29 scraped successfully with 1 articles.\n",
      "Page 1131 scraped successfully with 8 articles.\n",
      "Page 424 scraped successfully with 0 articles.\n",
      "Page 1136 scraped successfully with 8 articles.\n",
      "Page 428 scraped successfully with 0 articles.\n",
      "Page 785 scraped successfully with 1 articles.\n",
      "Page 69 scraped successfully with 0 articles.\n",
      "Page 699 scraped successfully with 0 articles.\n",
      "Page 417 scraped successfully with 0 articles.\n",
      "Page 82 scraped successfully with 1 articles.\n",
      "Page 862 scraped successfully with 0 articles.\n",
      "Page 63 scraped successfully with 0 articles.\n",
      "Page 61 scraped successfully with 0 articles.\n",
      "Page 86 scraped successfully with 1 articles.\n",
      "Page 430 scraped successfully with 0 articles.\n",
      "Page 418 scraped successfully with 1 articles.\n",
      "Page 90 scraped successfully with 0 articles.\n",
      "Page 10 scraped successfully with 1 articles.\n",
      "Page 83 scraped successfully with 0 articles.\n",
      "Page 80 scraped successfully with 1 articles.\n",
      "Page 16 scraped successfully with 0 articles.\n",
      "Page 431 scraped successfully with 1 articles.\n",
      "Page 1070 scraped successfully with 8 articles.\n",
      "Page 701 scraped successfully with 0 articles.\n",
      "Page 795 scraped successfully with 0 articles.\n",
      "Page 77 scraped successfully with 1 articles.\n",
      "Page 18 scraped successfully with 0 articles.\n",
      "Page 63 scraped successfully with 0 articles.\n",
      "Page 813 scraped successfully with 0 articles.\n",
      "Page 798 scraped successfully with 0 articles.\n",
      "Page 783 scraped successfully with 0 articles.\n",
      "Page 1063 scraped successfully with 8 articles.\n",
      "Page 788 scraped successfully with 0 articles.\n",
      "Page 427 scraped successfully with 0 articles.\n",
      "Page 68 scraped successfully with 0 articles.\n"
     ]
    }
   ],
   "source": [
    "import backoff\n",
    "\n",
    "@backoff.on_exception(backoff.expo,\n",
    "                      aiohttp.ClientError,\n",
    "                      max_tries=8,\n",
    "                      giveup=lambda e: e.status == 403)\n",
    "async def fetch(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        response.raise_for_status()\n",
    "        return await response.text()\n",
    "\n",
    "\n",
    "async def scrape_page(i, session):\n",
    "    pg = str(i)\n",
    "    try:\n",
    "        url = f\"https://cafe.naver.com/ArticleList.nhn?search.clubid=11262350&search.boardtype=L&search.totalCount=151&search.cafeId=11262350&search.page={pg}\"\n",
    "        html = await fetch(session, url)\n",
    "        soup = bs(html, \"lxml\")\n",
    "        articles = []\n",
    "        parsed_datas = soup.find_all(\"a\", {\"class\": \"article\"})\n",
    "        if not parsed_datas:\n",
    "            print(f\"No articles found on page {pg}\")\n",
    "            return []\n",
    "        for data in parsed_datas:\n",
    "            article_text = str(data).strip().replace(\"\\n\", \" \").replace(\"    \", \" \")\n",
    "            article_contents = re.findall(r'</span>(.*?)</a>', article_text)\n",
    "            if article_contents:\n",
    "                articles.append(article_contents[0].strip())\n",
    "        await asyncio.sleep(random.uniform(1, 5))  # Polite delay between requests\n",
    "        print(f\"Page {pg} scraped successfully with {len(articles)} articles.\")\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "async def main():\n",
    "    all_articles = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [scrape_page(i, session) for i in tqdm(range(1, 115))]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        for articles in results:\n",
    "            all_articles.extend(articles)\n",
    "\n",
    "    if all_articles:\n",
    "        print(f\"Total articles scraped: {len(all_articles)}\")\n",
    "        with open('text.txt', 'w', encoding='utf-8') as file:\n",
    "            for article in all_articles:\n",
    "                file.write(article + '\\n')\n",
    "    else:\n",
    "        print(\"No articles were scraped.\")\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "if loop.is_running():\n",
    "    asyncio.ensure_future(main())\n",
    "else:\n",
    "    loop.run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1496c72f-ede0-4670-8d97-e559744ff3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hannanum = Hannanum()\n",
    "words = []\n",
    "\n",
    "for article in articles:\n",
    "    nouns = hannanum.nouns(article)\n",
    "    words+=nouns\n",
    "\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b59f8-8af3-4bff-9e45-85fc5d8102fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7769ba-e090-45ca-9b6a-5efcbf1d30d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe02d8-231d-4b65-82a7-21dfcf743ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = WordCloud(font_path = r\"\", # Type your font's location\n",
    "    background_color=\"white\",\n",
    "    height = 1000,\n",
    "    width = 1000).generate_from_frequencies(counter)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca39c1f4-9c3e-416b-bc2a-6907a47d4b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
